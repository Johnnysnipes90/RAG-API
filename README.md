# RAG-API
A Retrieval-Augmented Generation (RAG) API built with FastAPI, FAISS/ChromaDB for vector search, and OpenAI LLM for response generation. Containerized with Docker and deployed on Kubernetes using Minikube.



# ğŸš€ RAG-API: Retrieval-Augmented Generation with FastAPI & Kubernetes

## ğŸ“Œ Overview
**RAG-API** is a **Retrieval-Augmented Generation (RAG)** system that efficiently retrieves relevant information from stored documents and generates responses using a **Large Language Model (LLM)**.  

The project is built using **FastAPI**, **FAISS/ChromaDB** for vector search, and **OpenAI/Hugging Face LLM** for response generation. The API is **containerized with Docker** and deployed locally on **Kubernetes (Minikube)**.

---

## ğŸ“‚ Project Structure

```
/rag-api
â”œâ”€â”€ api/                     # API Implementation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py               # FastAPI app entry point
â”‚   â”œâ”€â”€ routers/              # API route handlers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ingest.py         # Handles document ingestion
â”‚   â”‚   â”œâ”€â”€ query.py          # Handles user queries
â”‚
â”œâ”€â”€ k8s/                      # Kubernetes Deployment Files
â”‚   â”œâ”€â”€ deployment.yaml       # Defines API deployment
â”‚   â”œâ”€â”€ service.yaml          # Defines service (NodePort/LoadBalancer)
â”‚
â”œâ”€â”€ rag/                      # Retrieval-Augmented Generation (RAG) Pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm.py                # Handles LLM calls (OpenAI/HuggingFace)
â”‚   â”œâ”€â”€ rag_pipeline.py       # Manages RAG workflow (retrieval + response)
â”‚   â”œâ”€â”€ vectorstore.py        # Handles FAISS/ChromaDB storage
â”‚
â”œâ”€â”€ .env.example              # Environment variables template
â”œâ”€â”€ .gitignore                # Ignore unnecessary files
â”œâ”€â”€ Dockerfile                # Defines Docker container
â”œâ”€â”€ deploy.sh                 # Shell script to automate deployment
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # Documentation
```

---

## ğŸ› ï¸ Tech Stack
- **Backend:** FastAPI
- **Vector Storage:** FAISS / ChromaDB
- **LLM:** OpenAI API / HuggingFace
- **Containerization:** Docker
- **Deployment:** Kubernetes (Minikube)
- **Orchestration:** Shell script for automation

---

## ğŸ”§ Setup & Installation

### 1ï¸âƒ£ **Clone the Repository**
```bash
git clone https://github.com/your-username/rag-api.git
cd rag-api
```

### 2ï¸âƒ£ **Create a Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate  # Mac/Linux
venv\Scripts\activate     # Windows
```

### 3ï¸âƒ£ **Install Dependencies**
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ **Set Up Environment Variables**
Copy `.env.example` to `.env` and update values:
```bash
cp .env.example .env
```
Modify `.env` with API keys and database details.

### 5ï¸âƒ£ **Run the API Locally**
```bash
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```
The API will be available at **[http://localhost:8000](http://localhost:8000)**.

---

## ğŸš€ API Endpoints

| Method | Endpoint | Description |
|--------|---------|-------------|
| `POST` | `/ingest` | Uploads and stores documents for retrieval. |
| `POST` | `/query` | Accepts a query and returns an LLM-generated response. |
| `GET`  | `/health` | Checks API health status. |

### ğŸ›  **Testing API with cURL**
- **Ingest Documents**
```bash
curl -X POST "http://localhost:8000/ingest" -H "Content-Type: application/json" -d '{"text": "Your document content"}'
```
- **Query API**
```bash
curl -X POST "http://localhost:8000/query" -H "Content-Type: application/json" -d '{"query": "Your question"}'
```
- **Health Check**
```bash
curl -X GET "http://localhost:8000/health"
```

---

## ğŸ³ Docker Setup

### 1ï¸âƒ£ **Build Docker Image**
```bash
docker build -t rag-api .
```

### 2ï¸âƒ£ **Run Docker Container**
```bash
docker run -p 8000:8000 --env-file .env rag-api
```

---

## â˜¸ï¸ Kubernetes Deployment with Minikube

### 1ï¸âƒ£ **Start Minikube**
```bash
minikube start
```

### 2ï¸âƒ£ **Apply Kubernetes Manifests**
```bash
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
```

### 3ï¸âƒ£ **Check Pods & Services**
```bash
kubectl get pods
kubectl get services
```

### 4ï¸âƒ£ **Access API in Minikube**
For **LoadBalancer:**
```bash
minikube service rag-api-service
```
For **NodePort:**
```bash
kubectl port-forward service/rag-api-service 8000:80
```
API will be available at **[http://localhost:8000](http://localhost:8000)**.

---

## ğŸ“Š Future Improvements
âœ… Support multiple document formats (PDF, DOCX).  
âœ… Add Redis caching for query responses.  
âœ… Improve scalability with auto-scaling in Kubernetes.  

---

## ğŸ‘¨â€ğŸ’» Contributing
1. **Fork the repo**
2. **Create a new branch** (`feature-xyz`)
3. **Commit changes** (`git commit -m "Add feature XYZ"`)
4. **Push to GitHub** (`git push origin feature-xyz`)
5. **Open a Pull Request**

---

## ğŸ“œ License
This project is licensed under the **MIT License**.

---

## ğŸ™Œ Acknowledgments
Special thanks to **OpenAI**, **FAISS**, **HuggingFace**, and the **FastAPI** community.

---

ğŸ”— **GitHub Repository**: [your-github-link](https://github.com/your-username/rag-api)






